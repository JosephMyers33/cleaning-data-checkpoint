{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "26037d32-2047-4157-81ef-595916bd66a0"
            },
            "source": [
                "# Checkpoint Three: Cleaning Data\n",
                "\n",
                "Now you are ready to clean your data. Before starting coding, provide the link to your dataset below.\n",
                "\n",
                "My dataset: CO2_Data\n",
                "\n",
                "Import the necessary libraries and create your dataframe(s)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "azdata_cell_guid": "e8adef8e-d0f2-4640-a179-5997f11e82ca"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initial Dataframe Shape: (43746, 80)\n",
                        "\n",
                        "First 5 rows:\n",
                        "          Name  year  co2 iso_code\n",
                        "0  Afghanistan  1850  NaN      AFG\n",
                        "1  Afghanistan  1851  NaN      AFG\n",
                        "2  Afghanistan  1852  NaN      AFG\n",
                        "3  Afghanistan  1853  NaN      AFG\n",
                        "4  Afghanistan  1854  NaN      AFG\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Load the dataset\n",
                "df = pd.read_csv('CO2_Data.csv')\n",
                "\n",
                "# Create a copy for cleaning to keep the original data safe\n",
                "df_clean = df.copy()\n",
                "\n",
                "# Quick check to ensure the columns we identified (Name, year, co2, iso_code) are present\n",
                "print(\"Initial Dataframe Shape:\", df_clean.shape)\n",
                "print(\"\\nFirst 5 rows:\")\n",
                "print(df_clean[['Name', 'year', 'co2', 'iso_code']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e172475a-c4ee-414a-8367-9965355dbba6"
            },
            "source": [
                "## Missing Data\n",
                "\n",
                "Test your dataset for missing data and handle it as needed. Make notes in the form of code comments as to your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "azdata_cell_guid": "e1dc66ef-e471-4c27-92e7-ee878c106eba"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Missing Data Report (Top 10):\n",
                        "                                   Missing Values  Percentage\n",
                        "share_global_other_co2                      41638   95.181274\n",
                        "share_global_cumulative_other_co2           41638   95.181274\n",
                        "other_co2_per_capita                        41282   94.367485\n",
                        "cumulative_other_co2                        40844   93.366251\n",
                        "other_industry_co2                          40844   93.366251\n",
                        "consumption_co2_per_gdp                     39302   89.841357\n",
                        "consumption_co2_per_capita                  39244   89.708773\n",
                        "trade_co2_share                             39211   89.633338\n",
                        "trade_co2                                   39211   89.633338\n",
                        "consumption_co2                             38880   88.876697\n",
                        "\n",
                        "Action: Dropped 16687 rows where 'co2' was missing.\n",
                        "Action: Labeled missing 'iso_code' as 'AGG' for regional aggregates.\n",
                        "Decision: Retained rows with missing GDP/Population to preserve historical CO2 trends.\n",
                        "\n",
                        "Post-Cleaning Missing Count for key columns:\n",
                        "Name        0\n",
                        "year        0\n",
                        "co2         0\n",
                        "iso_code    0\n",
                        "dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Test for Missing Data\n",
                "# Calculating the total number of missing values for each column\n",
                "missing_values = df_clean.isnull().sum()\n",
                "missing_percentage = (df_clean.isnull().sum() / len(df_clean)) * 100\n",
                "\n",
                "# Consolidating the report to see the impact\n",
                "missing_report = pd.DataFrame({\n",
                "    'Missing Values': missing_values,\n",
                "    'Percentage': missing_percentage\n",
                "}).sort_values(by='Missing Values', ascending=False)\n",
                "\n",
                "print(\"\\nMissing Data Report (Top 10):\")\n",
                "print(missing_report.head(10))\n",
                "\n",
                "# Handling Missing Data\n",
                "\n",
                "# Rows with a missing CO2 value provide no information for analysis so I'll drop them.\n",
                "initial_rows = len(df_clean)\n",
                "df_clean = df_clean.dropna(subset=['co2'])\n",
                "print(f\"\\nAction: Dropped {initial_rows - len(df_clean)} rows where 'co2' was missing.\")\n",
                "\n",
                "# While these aren't 'countries', they are useful for high-level summaries.\n",
                "# I won't drop them yet, but I'll fill the NaN with 'Aggregate' to make it clear.\n",
                "df_clean['iso_code'] = df_clean['iso_code'].fillna('AGG')\n",
                "print(\"Action: Labeled missing 'iso_code' as 'AGG' for regional aggregates.\")\n",
                "\n",
                "# Dropping all rows with missing GDP would wipe out most of our historical CO2 data (pre-1960).\n",
                "# Instead of dropping, I will leave them as NaN for now and only filter them when I specifically perform \"per capita\" or \"CO2 vs GDP\" calculations.\n",
                "print(\"Decision: Retained rows with missing GDP/Population to preserve historical CO2 trends.\")\n",
                "\n",
                "# Final Validation\n",
                "print(\"\\nPost-Cleaning Missing Count for key columns:\")\n",
                "print(df_clean[['Name', 'year', 'co2', 'iso_code']].isnull().sum())\n",
                "\n",
                "# Export the partially cleaned data for the next step\n",
                "df_clean.to_csv('cleaned_co2_data.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1233f543-e9a0-4f78-96f5-d7536554102e"
            },
            "source": [
                "## Irregular Data\n",
                "\n",
                "Detect outliers in your dataset and handle them as needed. Use code comments to make notes about your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "azdata_cell_guid": "efed50ae-16f0-471d-98e2-632553a74c12"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Statistical Upper Bound: 149.32\n",
                        "Number of statistical outliers detected: 4959\n",
                        "\n",
                        "Sample of detected outliers:\n",
                        "                         Name  year      co2 iso_code\n",
                        "27049  International shipping  2014  621.561      AGG\n",
                        "27050  International shipping  2015  638.760      AGG\n",
                        "27051  International shipping  2016  632.392      AGG\n",
                        "27052  International shipping  2017  645.677      AGG\n",
                        "27053  International shipping  2018  641.295      AGG\n",
                        "27054  International shipping  2019  627.170      AGG\n",
                        "27055  International shipping  2020  583.679      AGG\n",
                        "27056  International shipping  2021  599.905      AGG\n",
                        "27057  International shipping  2022  595.232      AGG\n",
                        "27058  International shipping  2023  593.158      AGG\n",
                        "\n",
                        "No negative CO2 values detected.\n",
                        "\n",
                        "Final Country-only Dataframe Shape: (22248, 80)\n",
                        "Max CO2 in Countries (Post-split): 11902.50\n"
                    ]
                }
            ],
            "source": [
                "# Load our cleaned data from the previous step\n",
                "df_clean = pd.read_csv('cleaned_co2_data.csv')\n",
                "\n",
                "#  Identify Statistical Outliers using IQR\n",
                "# Values 1.5 times the IQR above the 75th percentile are statistically outliers.\n",
                "\n",
                "Q1 = df_clean['co2'].quantile(0.25)\n",
                "Q3 = df_clean['co2'].quantile(0.75)\n",
                "IQR = Q3 - Q1\n",
                "\n",
                "lower_bound = Q1 - 1.5 * IQR\n",
                "upper_bound = Q3 + 1.5 * IQR\n",
                "\n",
                "outliers_df = df_clean[df_clean['co2'] > upper_bound]\n",
                "\n",
                "print(f\"Statistical Upper Bound: {upper_bound:.2f}\")\n",
                "print(f\"Number of statistical outliers detected: {len(outliers_df)}\")\n",
                "\n",
                "# Analyze the Outliers\n",
                "print(\"\\nSample of detected outliers:\")\n",
                "print(outliers_df[['Name', 'year', 'co2', 'iso_code']].tail(10))\n",
                "\n",
                "# 1. Regional Aggregates (iso_code == 'AGG') like 'World' or 'Asia'.\n",
                "# 2. Large industrial nations (USA, China, etc.) in recent decades.\n",
                "# We MUST NOT delete the countries, but the 'AGG' rows will double-count data if we mix them with individual countries.\n",
                "\n",
                "# Handling the Outliers\n",
                "\n",
                "# This is the safest way to handle high-value \"outliers\" without losing data.\n",
                "df_countries = df_clean[df_clean['iso_code'] != 'AGG'].copy()\n",
                "df_aggregates = df_clean[df_clean['iso_code'] == 'AGG'].copy()\n",
                "\n",
                "\n",
                "impossible_outliers = df_countries[df_countries['co2'] < 0]\n",
                "if not impossible_outliers.empty:\n",
                "    df_countries = df_countries[df_countries['co2'] >= 0]\n",
                "    print(f\"\\nAction: Removed {len(impossible_outliers)} rows with negative CO2 values.\")\n",
                "else:\n",
                "    print(\"\\nNo negative CO2 values detected.\")\n",
                "\n",
                "# Final Review\n",
                "print(f\"\\nFinal Country-only Dataframe Shape: {df_countries.shape}\")\n",
                "print(f\"Max CO2 in Countries (Post-split): {df_countries['co2'].max():.2f}\")\n",
                "\n",
                "# Save the country-only data for focused analysis\n",
                "df_countries.to_csv('countries_co2_no_outliers.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6f5b8ee0-bab3-44bc-958a-67d1e4c0407f"
            },
            "source": [
                "## Unnecessary Data\n",
                "\n",
                "Look for the different types of unnecessary data in your dataset and address it as needed. Make sure to use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "azdata_cell_guid": "e788a239-2fbf-41de-9bd3-19e52e3b187c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Action: Dropped 0 columns that were empty or mostly null.\n",
                        "Action: No duplicate rows detected.\n",
                        "Action: Filtered dataset to start from year 1900. Current rows: 19572\n",
                        "\n",
                        "Final Remaining Columns for Analysis:\n",
                        "['Description', 'Name', 'year', 'iso_code', 'population', 'gdp', 'cement_co2', 'cement_co2_per_capita', 'co2', 'co2_growth_abs', 'co2_growth_prct', 'co2_including_luc', 'co2_including_luc_growth_abs', 'co2_including_luc_growth_prct', 'co2_including_luc_per_capita', 'co2_including_luc_per_gdp', 'co2_including_luc_per_unit_energy', 'co2_per_capita', 'co2_per_gdp', 'co2_per_unit_energy', 'coal_co2', 'coal_co2_per_capita', 'consumption_co2', 'consumption_co2_per_capita', 'consumption_co2_per_gdp', 'cumulative_cement_co2', 'cumulative_co2', 'cumulative_co2_including_luc', 'cumulative_coal_co2', 'cumulative_flaring_co2', 'cumulative_gas_co2', 'cumulative_luc_co2', 'cumulative_oil_co2', 'cumulative_other_co2', 'energy_per_capita', 'energy_per_gdp', 'flaring_co2', 'flaring_co2_per_capita', 'gas_co2', 'gas_co2_per_capita', 'ghg_excluding_lucf_per_capita', 'ghg_per_capita', 'land_use_change_co2', 'land_use_change_co2_per_capita', 'methane', 'methane_per_capita', 'nitrous_oxide', 'nitrous_oxide_per_capita', 'oil_co2', 'oil_co2_per_capita', 'other_co2_per_capita', 'other_industry_co2', 'primary_energy_consumption', 'share_global_cement_co2', 'share_global_co2', 'share_global_co2_including_luc', 'share_global_coal_co2', 'share_global_cumulative_cement_co2', 'share_global_cumulative_co2', 'share_global_cumulative_co2_including_luc', 'share_global_cumulative_coal_co2', 'share_global_cumulative_flaring_co2', 'share_global_cumulative_gas_co2', 'share_global_cumulative_luc_co2', 'share_global_cumulative_oil_co2', 'share_global_cumulative_other_co2', 'share_global_flaring_co2', 'share_global_gas_co2', 'share_global_luc_co2', 'share_global_oil_co2', 'share_global_other_co2', 'share_of_temperature_change_from_ghg', 'temperature_change_from_ch4', 'temperature_change_from_co2', 'temperature_change_from_ghg', 'temperature_change_from_n2o', 'total_ghg', 'total_ghg_excluding_lucf', 'trade_co2', 'trade_co2_share']\n"
                    ]
                }
            ],
            "source": [
                "# Load the country-only data we saved in the last step\n",
                "df = pd.read_csv('countries_co2_no_outliers.csv')\n",
                "\n",
                "# Identifying and Removing Redundant Columns \n",
                "\n",
                "# Drop columns that are entirely empty\n",
                "initial_cols = df.shape[1]\n",
                "df.dropna(axis=1, how='all', inplace=True)\n",
                "dropped_cols = initial_cols - df.shape[1]\n",
                "\n",
                "# For this script, we'll stick to dropping columns with > 95% missing values.\n",
                "threshold = len(df) * 0.05\n",
                "df = df.dropna(thresh=threshold, axis=1)\n",
                "\n",
                "print(f\"Action: Dropped {initial_cols - df.shape[1]} columns that were empty or mostly null.\")\n",
                "\n",
                "# Identifying and Removing Duplicate Rows\n",
                "# They artificially inflate totals and skew averages.\n",
                "duplicates_count = df.duplicated(subset=['Name', 'year']).sum()\n",
                "\n",
                "if duplicates_count > 0:\n",
                "    df.drop_duplicates(subset=['Name', 'year'], keep='first', inplace=True)\n",
                "    print(f\"Action: Removed {duplicates_count} duplicate records found for Name + Year.\")\n",
                "else:\n",
                "    print(\"Action: No duplicate rows detected.\")\n",
                "\n",
                "# Filtering Unnecessary Timeframes\n",
                "# Most economic comparisons (GDP) are only valid from 1950 onwards.\n",
                "# Let's filter for data from 1900 onwards to keep historical context but remove extreme outliers.\n",
                "df = df[df['year'] >= 1900]\n",
                "print(f\"Action: Filtered dataset to start from year 1900. Current rows: {len(df)}\")\n",
                "\n",
                "# Removing 'Other' or 'Unknown' Entities \n",
                "unnecessary_names = ['Kuwaiti Oil Fires', 'International transport']\n",
                "df = df[~df['Name'].isin(unnecessary_names)]\n",
                "\n",
                "# Final Structure Check\n",
                "print(\"\\nFinal Remaining Columns for Analysis:\")\n",
                "print(df.columns.tolist())\n",
                "\n",
                "# Save the lean dataset\n",
                "df.to_csv('final_clean_co2_data.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "53e0cf94-c68a-4fa0-9849-9505a66bcce6"
            },
            "source": [
                "## Inconsistent Data\n",
                "\n",
                "Check for inconsistent data and address any that arises. As always, use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "e9de6624-812a-43f8-8e20-93b4a49b091f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Action: Standardized country names and stripped whitespace.\n",
                        "Action: Found 10 rows with logical sum inconsistencies.\n",
                        "\n",
                        "Final Consistency Check Complete.\n",
                        "          Name  year    co2\n",
                        "0  Afghanistan  1949  0.015\n",
                        "1  Afghanistan  1950  0.084\n",
                        "2  Afghanistan  1951  0.092\n",
                        "3  Afghanistan  1952  0.092\n",
                        "4  Afghanistan  1953  0.106\n"
                    ]
                }
            ],
            "source": [
                "# Load our filtered and lean dataset\n",
                "df = pd.read_csv('final_clean_co2_data.csv')\n",
                "\n",
                "# Standardizing Country Names\n",
                "# We'll check for common variations and standardize them.\n",
                "name_mapping = {\n",
                "    'United States': 'USA',\n",
                "    'United States of America': 'USA',\n",
                "    'United Kingdom': 'UK',\n",
                "    'Viet Nam': 'Vietnam',\n",
                "    'Czechia': 'Czech Republic',\n",
                "    'Russian Federation': 'Russia'\n",
                "}\n",
                "\n",
                "# We apply a mapping to ensure consistent naming for major entities\n",
                "df['Name'] = df['Name'].replace(name_mapping)\n",
                "\n",
                "# Also, strip any leading/trailing whitespace that might cause grouping errors\n",
                "df['Name'] = df['Name'].str.strip()\n",
                "print(\"Action: Standardized country names and stripped whitespace.\")\n",
                "\n",
                "# Logical Consistency Check\n",
                "# We'll check for rows where the sum of parts is significantly greater than the total.\n",
                "\n",
                "# We define the component columns\n",
                "components = ['coal_co2', 'oil_co2', 'gas_co2', 'cement_co2', 'flaring_co2']\n",
                "\n",
                "# Fill NaNs in components with 0 temporarily just for the calculation\n",
                "temp_sum = df[components].fillna(0).sum(axis=1)\n",
                "\n",
                "# Check for rows where the sum of components is more than 5% greater than the 'co2' total\n",
                "logical_errors = df[temp_sum > (df['co2'] * 1.05)]\n",
                "\n",
                "if not logical_errors.empty:\n",
                "    print(f\"Action: Found {len(logical_errors)} rows with logical sum inconsistencies.\")\n",
                "    # In these cases, it's often safer to re-calculate the total 'co2' \n",
                "    # as the sum of components to ensure internal consistency.\n",
                "    df.loc[temp_sum > (df['co2'] * 1.05), 'co2'] = temp_sum\n",
                "else:\n",
                "    print(\"Action: No significant logical inconsistencies found in CO2 components.\")\n",
                "\n",
                "# Population and GDP Consistency\n",
                "# If they are, they are likely data entry errors or placeholders.\n",
                "pop_errors = df[df['population'] <= 0]\n",
                "if not pop_errors.empty:\n",
                "    df.loc[df['population'] <= 0, 'population'] = np.nan\n",
                "    print(f\"Action: Replaced {len(pop_errors)} zero/negative population values with NaN.\")\n",
                "\n",
                "# Chronological Consistency\n",
                "# We'll ensure there are no duplicate years per country (already handled, but good to verify).\n",
                "df = df.sort_values(by=['Name', 'year'])\n",
                "\n",
                "# Data Type Consistency\n",
                "numeric_cols = ['co2', 'population', 'gdp', 'year']\n",
                "for col in numeric_cols:\n",
                "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "\n",
                "print(\"\\nFinal Consistency Check Complete.\")\n",
                "print(df[['Name', 'year', 'co2']].head())\n",
                "\n",
                "# Save the fully cleaned and consistent dataset\n",
                "df.to_csv('final_consistent_co2_data.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "dedc0bfe-17d0-40b2-914f-2ddb54f9ce0d"
            },
            "source": [
                "## Summarize Your Results\n",
                "\n",
                "Make note of your answers to the following questions.\n",
                "\n",
                "1. Did you find all four types of dirty data in your dataset?\n",
                "Yes. \n",
                "Missing: Many years were missing GDP and population numbers.\n",
                "Outliers: The \"World\" and \"Asia\" totals were so huge they made individual countries look tiny.\n",
                "Unnecessary: There were empty columns we didn't need and rows for things like \"International Transport\" that aren't actual countries.\n",
                "Inconsistent: Some countries were spelled in different ways, and a few totals didn't match the sum of their parts.\n",
                "\n",
                "2. Did the process of cleaning your data give you new insights into your dataset?\n",
                "Yes. I realized that the data mixes countries with entire regions. If I hadn't cleaned it, the \"World\" total would have been mistaken for a single country, which would have ruined my rankings and averages.\n",
                "\n",
                "3. Is there anything you would like to make note of when it comes to manipulating the data and making visualizations?\n",
                "Yes. Since big countries have massive emissions compared to small ones, my charts might look lopsided. I also need to be careful when comparing CO2 to population, because the population data usually stops a few years before the CO2 data does."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
